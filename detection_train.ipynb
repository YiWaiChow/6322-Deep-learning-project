{"cells":[{"cell_type":"markdown","metadata":{"id":"IOn1Lt81BFRd"},"source":["### **Import and Model**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Reference:\n","\n","voxel51, “Fiftyone-examples/pytorch_detection_training.ipynb at master · Voxel51/fiftyone-examples,” GitHub, 07-Sep-2022. [Online]. Available: https://github.com/voxel51/fiftyone-examples/blob/master/examples/pytorch_detection_training.ipynb."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install fiftyone\n","!pip install torch torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%shell\n","\n","# Download TorchVision repo to use some files from\n","# references/detection\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.3.0\n","\n","cp references/detection/utils.py ../\n","cp references/detection/transforms.py ../\n","cp references/detection/coco_eval.py ../\n","cp references/detection/engine.py ../\n","cp references/detection/coco_utils.py ../"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","import torchvision\n","import torchvision.ops as ops\n","import torch.optim as optim\n","from torchvision import datasets\n","from torchvision import transforms\n","import torch.nn as nn\n","from torchvision.transforms import ToTensor\n","\n","class Patch_Embedding(nn.Module):\n","    def __init__(self, channel, embed_dim, patch_dim):\n","        super().__init__()\n","        self.in_dim = channel\n","        self.out_dim = embed_dim\n","\n","        self.P = patch_dim\n","\n","        # this outputs a shape of Batch size, embedding dimension, H, W\n","        self.linear = nn.Conv2d(\n","            channel, embed_dim, kernel_size=patch_dim, stride=patch_dim, bias=True)\n","        # self.norm = nn.LayerNorm([height/self.P, width/self.P, embed_dim])\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","\n","        # flatten it into 2d, so H and W collapse into number of patches, then we swap the shape\n","        # from [B, ED, H,W] -> [B, ED, number of patches] -> [B, number of patches, ED]\n","        # this is done to follow the convention of the paper, where the embedding dimension is the last dimension\n","\n","        x = self.linear(x)\n","\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","        # output shape should be [B, Number of patches, ED], where number of patches should be HW/4*2\n","\n","        return x\n","\n","\n","# Spatial-Reduction Attention\n","class SRAttention(nn.Module):\n","    def __init__(self, num_heads, channels, height, width, reduction_ratio, batch_size):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.head_dimension = channels//self.num_heads\n","\n","        self.c = channels\n","\n","        # the Weight is Ci X d Head, so the input dimension should be c and the output should be d head\n","        self.L = nn.Linear(self.c,\n","                           self.head_dimension)\n","        self.sr = SR(height, width, channels,\n","                     reduction_ratio, batch_size)\n","        #  Wo has size Ci X Ci, this is becasuse d head = Ci/Ni, after concatnating N Ci, the dimension becomes Ci.\n","        self.L2 = nn.Linear(self.c, self.c)\n","\n","    def forward(self, query, key, value):\n","        SRA = None\n","        for i in range(self.num_heads):\n","            # HW x d_head\n","            qi = self.L(query)\n","            # HW/R^2 x d_head\n","            srk = self.L(self.sr(key))\n","            # HW/R^2 x d_head\n","            srv = self.L(self.sr(value))\n","            # attention at stage i\n","            # HW X d_head @ d_head X HW/R^2 @ HW/R^2 x d_head = > HW X d_head <--- the shape of the A_i\n","            Ai = ((torch.softmax(qi.clone().detach()@srk.clone().detach().transpose(1, 2) /\n","                                (self.head_dimension**0.5), dim=1))@srv)\n","            if(SRA is None):\n","                SRA = Ai\n","            else:\n","\n","                SRA = torch.cat((SRA, Ai), dim=2)\n","\n","        # SRA after concatinating should be HW X D_head*Ni -> HW X Ci\n","        SRA = self.L2(SRA)\n","\n","        return SRA\n","\n","\n","# Spatial Reduction\n","# SR(x) = Norm(Reshape(x,Ri)W^s)\n","class SR(nn.Module):\n","    def __init__(self, height, width, channels, reduction_ratio, batch_size):\n","        super().__init__()\n","        self.H = height\n","        self.W = width\n","        self.C = channels\n","        self.B = batch_size\n","        self.R = reduction_ratio\n","        # after reshaping x into HW/R^2 X R^2C, it takes in R^2C and projects to Ci\n","        self.linear_projection = nn.Linear(self.R**2*self.C, self.C)\n","        # then re layer norm on the number of channels\n","        self.norm = nn.LayerNorm(self.C)\n","\n","    def forward(self, x):\n","        # reduced the sptial scale of x\n","        # by reshaping the sequence into size HW/R^2 X R^2C at stage i\n","\n","        reduced_x = torch.reshape(\n","            x, [self.B, self.H*self.W//(self.R**2), (self.R**2*self.C)]).clone()\n","        new_x = self.linear_projection(reduced_x)\n","        new_x = self.norm(new_x)\n","        # output should be of size HW/R^2 x CI\n","\n","        return new_x\n","\n","\n","class Feed_Forward(nn.Module):\n","    def __init__(self, in_size, hidden_size, out_size):\n","        super().__init__()\n","        self.l1 = nn.Linear(in_size, hidden_size)\n","        self.relu = nn.ReLU(inplace=False)\n","        self.l2 = nn.Linear(hidden_size, out_size)\n","\n","    def forward(self, x):\n","        x = self.l1(x)\n","        x = self.relu(x)\n","        x = self.l2(x)\n","        return x\n","\n","\n","class Transformer_Encoder(nn.Module):\n","    def __init__(self, height, width, channels, reduction_ratio, patch_dim, batch_size, num_heads):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.norm1 = nn.LayerNorm(channels)\n","        self.a = SRAttention(self.num_heads, channels,\n","                             height//patch_dim, width//patch_dim, reduction_ratio, batch_size)\n","        self.norm2 = nn.LayerNorm(channels)\n","        self.ff = Feed_Forward(channels, channels//2, channels)\n","\n","    def forward(self, x):\n","        n1 = self.norm1(x)\n","        a = self.a(n1, n1, n1)\n","        x1 = torch.add(x, a)\n","        n2 = self.norm2(x1)\n","        ff = self.ff(n2)\n","        x2 = torch.add(x1, ff)\n","        return x2\n","\n","\n","class Stage_Module(nn.Module):\n","    # # # added parameter patch_dim\n","    def __init__(self, channels, embedding_dim, Height, Width, reduction_ratio, patch_dim, batch_size, num_heads):\n","        super().__init__()\n","        self.H = Height\n","        self.W = Width\n","        self.out_dim = embedding_dim\n","        self.P = patch_dim\n","        self.B = batch_size\n","        self.PE = Patch_Embedding(channels, embedding_dim, patch_dim)\n","        self.TE = Transformer_Encoder(\n","            Height, Width, embedding_dim, reduction_ratio, patch_dim, batch_size, num_heads)\n","\n","    def forward(self, x):\n","        x = self.PE(x)\n","        x = self.TE(x)\n","        # # # reshape to H(i-1)/P x W(i-1)/P x ED as output\n","        x = torch.reshape(x.clone(), [self.B, self.H//self.P,\n","                              self.W//self.P, self.out_dim]).permute([0, 3, 1, 2])\n","        return x\n","\n","\n","class PVT(nn.Module):\n","    def __init__(self, channels, height, width, batch_size):\n","        super().__init__()\n","        # input at stage 1 is H X W X 3\n","\n","        self.stg1 = Stage_Module(channels, 64, height,\n","                                 width, reduction_ratio=8, patch_dim=4, batch_size=batch_size, num_heads=1)\n","        \n","        self.stg2 = Stage_Module(\n","            64, 128, height//4, width//4, reduction_ratio=4, patch_dim=2, batch_size=batch_size, num_heads=2)\n","        \n","        self.stg3 = Stage_Module(\n","            128, 256, height//8, width//8, reduction_ratio=2, patch_dim=2, batch_size=batch_size, num_heads=4)\n","        \n","        self.stg4 = Stage_Module(256, 512, height//16,\n","                                 width//16, reduction_ratio=1, patch_dim=2, batch_size=batch_size, num_heads=8)\n","        \n","\n","        self.head = nn.linear(512)\n","\n","    def forward(self, x):\n","\n","        x = self.stg1(x)\n","\n","        x = self.stg2(x)\n","\n","        x = self.stg3(x)\n","\n","        x = self.stg4(x)\n","\n","        return x\n","\n","\n","class classification_pvt(nn.Module):\n","    def __init__(self, channels, height, width, batch_size, num_classes):\n","        super().__init__()\n","        # input at stage 1 is H X W X 3\n","\n","        # # # will look to clean it up later\n","        # # # maybe we should only pass the original height and width for all stages, will verify it tmr\n","        self.stg1 = Stage_Module(channels, 64, height,\n","                                 width, reduction_ratio=8, patch_dim=4, batch_size=batch_size, num_heads=1)\n","        self.stg2 = Stage_Module(\n","            64, 128, height//4, width//4, reduction_ratio=4, patch_dim=2, batch_size=batch_size, num_heads=2)\n","        self.stg3 = Stage_Module(\n","            128, 256, height//8, width//8, reduction_ratio=2, patch_dim=2, batch_size=batch_size, num_heads=4)\n","        self.stg4 = Stage_Module(256, 512, height//16,\n","                                 width//16, reduction_ratio=1, patch_dim=2, batch_size=batch_size, num_heads=8)\n","\n","        self.head = nn.Linear(7*7*512, 128)\n","        self.head2 = nn.Linear(128, 100)\n","        self.relu = nn.ReLU(inplace=False)\n","\n","    def forward(self, x):\n","\n","        x = self.stg1(x)\n","\n","        x = self.stg2(x)\n","\n","        x = self.stg3(x)\n","\n","        x = self.stg4(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"i1Pa931rBMQd"},"source":["### **COCO2017 (detection)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import fiftyone.utils.coco as fouc\n","import fiftyone.zoo as foz\n","import fiftyone as fo\n","from PIL import Image\n","\n","\n","class FiftyOneTorchDataset(torch.utils.data.Dataset):\n","    def __init__(\n","        self,\n","        fiftyone_dataset,\n","        transforms=None,\n","        gt_field=\"ground_truth\",\n","        classes=None,\n","    ):\n","        self.samples = fiftyone_dataset\n","        self.transforms = transforms\n","        self.gt_field = gt_field\n","\n","        self.img_paths = self.samples.values(\"filepath\")\n","\n","        self.classes = classes\n","        if not self.classes:\n","            self.classes = self.samples.distinct(\n","                \"%s.detections.label\" % gt_field\n","            )\n","\n","        if self.classes[0] != \"background\":\n","            self.classes = [\"background\"] + self.classes\n","\n","        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        sample = self.samples[img_path]\n","        metadata = sample.metadata\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        boxes = []\n","        labels = []\n","        area = []\n","        iscrowd = []\n","        detections = sample[self.gt_field].detections\n","        for det in detections:\n","            category_id = self.labels_map_rev[det.label]\n","            coco_obj = fouc.COCOObject.from_label(\n","                det, metadata, category_id=category_id,\n","            )\n","            x, y, w, h = coco_obj.bbox\n","            boxes.append([x, y, x + w, y + h])\n","            labels.append(coco_obj.category_id)\n","            area.append(coco_obj.area)\n","            iscrowd.append(coco_obj.iscrowd)\n","\n","        target = {}\n","        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n","        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n","        target[\"image_id\"] = torch.as_tensor([idx])\n","        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n","        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def get_classes(self):\n","        return self.classes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coco_train = foz.load_zoo_dataset(\n","    \"coco-2017\",\n","    split = \"train\",\n","    label_types=[\"detections\"])\n","\n","coco_val = foz.load_zoo_dataset(\n","    \"coco-2017\",\n","    split = \"validation\",\n","    label_types=[\"detections\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coco_train.compute_metadata()\n","coco_val.compute_metadata()\n","\n","batch_size = 16"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import transforms as T\n","\n","train_transforms = T.Compose([T.ToTensor(), T.RandomHorizontalFlip(0.5)])\n","test_transforms = T.Compose([T.ToTensor()])\n","\n","torch_dataset = FiftyOneTorchDataset(coco_train, train_transforms)\n","\n","torch_dataset_test = FiftyOneTorchDataset(coco_val, test_transforms)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import functions from the torchvision references we cloned\n","from engine import train_one_epoch, evaluate\n","\n","def do_training(model, torch_dataset, torch_dataset_test, num_epochs=4):\n","    # define training and validation data loaders\n","    data_loader = torch.utils.data.DataLoader(\n","        torch_dataset, batch_size=16, shuffle=True, num_workers=2,\n","        collate_fn=utils.collate_fn, drop_last=True)\n","    \n","    data_loader_test = torch.utils.data.DataLoader(\n","        torch_dataset_test, batch_size=16, shuffle=False, num_workers=2,\n","        collate_fn=utils.collate_fn, drop_last=True)\n","\n","    # train on the GPU or on the CPU, if a GPU is not available\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    print(\"Using device %s\" % device)\n","\n","    # move model to the right device\n","    model.to(device)\n","\n","    # construct an optimizer\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.005,\n","                                momentum=0.9, weight_decay=0.0005)\n","    # and a learning rate scheduler\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                    step_size=3,\n","                                                    gamma=0.1)\n","\n","    for epoch in range(num_epochs):\n","        # train for one epoch, printing every 10 iterations\n","        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)\n","\n","        # update the learning rate\n","        lr_scheduler.step()\n","        # evaluate on the test dataset\n","        evaluate(model, data_loader_test, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","def get_model(num_classes):\n","    # load a model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    \n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    print(in_features)\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = get_model(80)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_path = './ckpt_coco2017'\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","net = classification_pvt(3, 640, 480, batch_size, 80)\n","net.to(device)\n","net.load_state_dict(torch.load('./ckpt_cifar100/cifar100_new_params.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_final = nn.Sequential(net, model)\n","do_training(model_final, torch_dataset, torch_dataset_test, num_epochs=30)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM5K2/fq8V4eZKyuSassdTV","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
