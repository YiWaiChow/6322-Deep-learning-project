{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmgbjl83AdvV"
      },
      "source": [
        "### **Import and Model**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reference:\n",
        "\n",
        "“Training a classifier,” Training a Classifier - PyTorch Tutorials 2.0.0+cu117 documentation. [Online]. Available: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrFgisLjAZcj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Patch_Embedding(nn.Module):\n",
        "    def __init__(self, channel, embed_dim, patch_dim):\n",
        "        super().__init__()\n",
        "        self.in_dim = channel\n",
        "        self.out_dim = embed_dim\n",
        "\n",
        "        self.P = patch_dim\n",
        "\n",
        "        # this outputs a shape of Batch size, embedding dimension, H, W\n",
        "        self.linear = nn.Conv2d(\n",
        "            channel, embed_dim, kernel_size=patch_dim, stride=patch_dim, bias=True)\n",
        "        # self.norm = nn.LayerNorm([height/self.P, width/self.P, embed_dim])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # flatten it into 2d, so H and W collapse into number of patches, then we swap the shape\n",
        "        # from [B, ED, H,W] -> [B, ED, number of patches] -> [B, number of patches, ED]\n",
        "        # this is done to follow the convention of the paper, where the embedding dimension is the last dimension\n",
        "\n",
        "        x = self.linear(x)\n",
        "\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.norm(x)\n",
        "        # output shape should be [B, Number of patches, ED], where number of patches should be HW/4*2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Spatial-Reduction Attention\n",
        "class SRAttention(nn.Module):\n",
        "    def __init__(self, num_heads, channels, height, width, reduction_ratio, batch_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dimension = channels//self.num_heads\n",
        "\n",
        "        self.c = channels\n",
        "\n",
        "        # the Weight is Ci X d Head, so the input dimension should be c and the output should be d head\n",
        "        self.L = nn.Linear(self.c,\n",
        "                           self.head_dimension)\n",
        "        self.sr = SR(height, width, channels,\n",
        "                     reduction_ratio, batch_size)\n",
        "        #  Wo has size Ci X Ci, this is becasuse d head = Ci/Ni, after concatnating N Ci, the dimension becomes Ci.\n",
        "        self.L2 = nn.Linear(self.c, self.c)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        SRA = None\n",
        "        for i in range(self.num_heads):\n",
        "            # HW x d_head\n",
        "            qi = self.L(query)\n",
        "            # HW/R^2 x d_head\n",
        "            srk = self.L(self.sr(key))\n",
        "            # HW/R^2 x d_head\n",
        "            srv = self.L(self.sr(value))\n",
        "            # attention at stage i\n",
        "            # HW X d_head @ d_head X HW/R^2 @ HW/R^2 x d_head = > HW X d_head <--- the shape of the A_i\n",
        "            Ai = (torch.softmax(qi@srk.transpose(1, 2) /\n",
        "                                (self.head_dimension**0.5), dim=1))@srv\n",
        "            if(SRA is None):\n",
        "                SRA = Ai\n",
        "            else:\n",
        "\n",
        "                SRA = torch.cat((SRA, Ai), dim=2)\n",
        "\n",
        "        # SRA after concatinating should be HW X D_head*Ni -> HW X Ci\n",
        "        SRA = self.L2(SRA)\n",
        "\n",
        "        return SRA\n",
        "\n",
        "\n",
        "# Spatial Reduction\n",
        "# SR(x) = Norm(Reshape(x,Ri)W^s)\n",
        "class SR(nn.Module):\n",
        "    def __init__(self, height, width, channels, reduction_ratio, batch_size):\n",
        "        super().__init__()\n",
        "        self.H = height\n",
        "        self.W = width\n",
        "        self.C = channels\n",
        "        self.B = batch_size\n",
        "        self.R = reduction_ratio\n",
        "        # after reshaping x into HW/R^2 X R^2C, it takes in R^2C and projects to Ci\n",
        "        self.linear_projection = nn.Linear(self.R**2*self.C, self.C)\n",
        "        # then re layer norm on the number of channels\n",
        "        self.norm = nn.LayerNorm(self.C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # reduced the sptial scale of x\n",
        "        # by reshaping the sequence into size HW/R^2 X R^2C at stage i\n",
        "\n",
        "        reduced_x = torch.reshape(\n",
        "            x, [self.B, self.H*self.W//(self.R**2), (self.R**2*self.C)])\n",
        "        new_x = self.linear_projection(reduced_x)\n",
        "        new_x = self.norm(new_x)\n",
        "        # output should be of size HW/R^2 x CI\n",
        "\n",
        "        return new_x\n",
        "\n",
        "\n",
        "class Feed_Forward(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size, out_size):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(in_size, hidden_size)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.l2 = nn.Linear(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.l2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer_Encoder(nn.Module):\n",
        "    def __init__(self, height, width, channels, reduction_ratio, patch_dim, batch_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.norm1 = nn.LayerNorm(channels)\n",
        "        self.a = SRAttention(self.num_heads, channels,\n",
        "                             height//patch_dim, width//patch_dim, reduction_ratio, batch_size)\n",
        "        self.norm2 = nn.LayerNorm(channels)\n",
        "        self.ff = Feed_Forward(channels, channels//2, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n1 = self.norm1(x)\n",
        "        a = self.a(n1, n1, n1)\n",
        "        x = torch.add(x, a)\n",
        "        n2 = self.norm2(x)\n",
        "        ff = self.ff(n2)\n",
        "        x = torch.add(x, ff)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Stage_Module(nn.Module):\n",
        "    def __init__(self, channels, embedding_dim, Height, Width, reduction_ratio, patch_dim, batch_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.H = Height\n",
        "        self.W = Width\n",
        "        self.out_dim = embedding_dim\n",
        "        self.P = patch_dim\n",
        "        self.B = batch_size\n",
        "        self.PE = Patch_Embedding(channels, embedding_dim, patch_dim)\n",
        "        self.TE = Transformer_Encoder(\n",
        "            Height, Width, embedding_dim, reduction_ratio, patch_dim, batch_size, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.PE(x)\n",
        "        x = self.TE(x)\n",
        "        # # # reshape to H(i-1)/P x W(i-1)/P x ED as output\n",
        "        x = torch.reshape(x, [self.B, self.H//self.P,\n",
        "                              self.W//self.P, self.out_dim]).permute([0, 3, 1, 2])\n",
        "        return x\n",
        "\n",
        "class classification_pvt(nn.Module):\n",
        "    def __init__(self, channels, height, width, batch_size, num_classes):\n",
        "        super().__init__()\n",
        "        # input at stage 1 is H X W X 3\n",
        "\n",
        "        self.output_H = height//32\n",
        "        self.output_W = width//32\n",
        "\n",
        "        self.stg1 = Stage_Module(channels, 64, height,\n",
        "                                 width, reduction_ratio=8, patch_dim=4, batch_size=batch_size, num_heads=1)\n",
        "        self.stg2 = Stage_Module(\n",
        "            64, 128, height//4, width//4, reduction_ratio=4, patch_dim=2, batch_size=batch_size, num_heads=2)\n",
        "        self.stg3 = Stage_Module(\n",
        "            128, 256, height//8, width//8, reduction_ratio=2, patch_dim=2, batch_size=batch_size, num_heads=4)\n",
        "        self.stg4 = Stage_Module(256, 512, height//16,\n",
        "                                 width//16, reduction_ratio=1, patch_dim=2, batch_size=batch_size, num_heads=8)\n",
        "\n",
        "        self.head = nn.Linear(self.output_H*self.output_W*512, 128)\n",
        "        self.head2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.stg1(x)\n",
        "\n",
        "        x = self.stg2(x)\n",
        "\n",
        "        x = self.stg3(x)\n",
        "\n",
        "        x = self.stg4(x).permute([0, 2, 3, 1])\n",
        "\n",
        "        x = x.view(-1, self.output_H*self.output_W*512)\n",
        "        x = self.head(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.head2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Zkmyf-A6NX"
      },
      "source": [
        "### **CIFAR100**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd7BOvkpA377"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((224, 224)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsZlhujAA73J"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "net = classification_pvt(3, 224, 224, batch_size, 100)\n",
        "net.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(net.parameters(), lr=5e-5, betas=[0.9, 0.999], weight_decay=1e-8)\n",
        "#optimizer = optim.AdamW(net.parameters(), lr=1e-3, betas=[0.9, 0.999], weight_decay=5e-2) # hyperparameters specified in the paper\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y69P5uQPA9C1"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "los4qODoA90Y"
      },
      "outputs": [],
      "source": [
        "save_path = '../ckpt_cifar100/'\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss_train = 0.0\n",
        "    correct_pred = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        images, labels = data\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = net(images)\n",
        "        loss = loss_fn(pred, labels)\n",
        "        torch.autograd.set_detect_anomaly(True) # for debugging\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_train += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "      torch.save(net.state_dict(), save_path + f'{epoch+1}.pth')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "            pred = net(images)\n",
        "            _, predictions = torch.max(pred.data, 1)\n",
        "            correct_pred += (predictions == labels).sum().item()\n",
        "    \n",
        "    print(f'Epoch {epoch + 1} -- loss: {loss_train/(50000//batch_size)}')\n",
        "    print(f'---------- testing accuracy: {correct_pred/(batch_size*(10000//batch_size))}')\n",
        "    print('#################################################')\n",
        "\n",
        "print('Training completed')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
