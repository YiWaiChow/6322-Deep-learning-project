{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWYSWs6/eMeqzRv9oT3+hC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### **Import and Model**"],"metadata":{"id":"jmgbjl83AdvV"}},{"cell_type":"markdown","source":["Reference:\n","\n","“Training a classifier,” Training a Classifier - PyTorch Tutorials 2.0.0+cu117 documentation. [Online]. Available: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html."],"metadata":{"id":"cfOiFh04Qvxm"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","import torchvision\n","from torchvision import datasets\n","from torchvision import transforms\n","import torch.nn as nn\n","from torchvision.transforms import ToTensor"],"metadata":{"id":"RrFgisLjAZcj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **CIFAR100**"],"metadata":{"id":"Z0Zkmyf-A6NX"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","from torchvision import transforms\n","import torch.nn as nn\n","\n","\n","class Patch_Embedding(nn.Module):\n","    def __init__(self, channel, embed_dim, patch_dim):\n","        super().__init__()\n","        self.in_dim = channel\n","        self.out_dim = embed_dim\n","\n","        self.P = patch_dim\n","\n","        # this outputs a shape of Batch size, embedding dimension, H, W\n","        self.linear = nn.Conv2d(\n","            channel, embed_dim, kernel_size=patch_dim, stride=patch_dim, bias=True)\n","        # self.norm = nn.LayerNorm([height/self.P, width/self.P, embed_dim])\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","\n","        # flatten it into 2d, so H and W collapse into number of patches, then we swap the shape\n","        # from [B, ED, H,W] -> [B, ED, number of patches] -> [B, number of patches, ED]\n","        # this is done to follow the convention of the paper, where the embedding dimension is the last dimension\n","\n","        x = self.linear(x)\n","\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","        # output shape should be [B, Number of patches, ED], where number of patches should be HW/4*2\n","\n","        return x\n","\n","\n","# Spatial-Reduction Attention\n","class SRAttention(nn.Module):\n","    def __init__(self, num_heads, channels, height, width, reduction_ratio, batch_size):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.head_dimension = channels//self.num_heads\n","\n","        self.c = channels\n","\n","        # the Weight is Ci X d Head, so the input dimension should be c and the output should be d head\n","        self.L = nn.Linear(self.c,\n","                           self.head_dimension)\n","        self.sr = SR(height, width, channels,\n","                     reduction_ratio, batch_size)\n","        #  Wo has size Ci X Ci, this is becasuse d head = Ci/Ni, after concatnating N Ci, the dimension becomes Ci.\n","        self.L2 = nn.Linear(self.c, self.c)\n","\n","    def forward(self, query, key, value):\n","        SRA = None\n","        for i in range(self.num_heads):\n","            # HW x d_head\n","            qi = self.L(query)\n","            # HW/R^2 x d_head\n","            srk = self.L(self.sr(key))\n","            # HW/R^2 x d_head\n","            srv = self.L(self.sr(value))\n","            # attention at stage i\n","            # HW X d_head @ d_head X HW/R^2 @ HW/R^2 x d_head = > HW X d_head <--- the shape of the A_i\n","            Ai = (torch.softmax(qi@srk.transpose(1, 2) /\n","                                (self.head_dimension**0.5), dim=1))@srv\n","            if(SRA is None):\n","                SRA = Ai\n","            else:\n","\n","                SRA = torch.cat((SRA, Ai), dim=2)\n","\n","        # SRA after concatinating should be HW X D_head*Ni -> HW X Ci\n","        SRA = self.L2(SRA)\n","\n","        return SRA\n","\n","\n","# Spatial Reduction\n","# SR(x) = Norm(Reshape(x,Ri)W^s)\n","class SR(nn.Module):\n","    def __init__(self, height, width, channels, reduction_ratio, batch_size):\n","        super().__init__()\n","        self.H = height\n","        self.W = width\n","        self.C = channels\n","        self.B = batch_size\n","        self.R = reduction_ratio\n","        # after reshaping x into HW/R^2 X R^2C, it takes in R^2C and projects to Ci\n","        self.linear_projection = nn.Linear(self.R**2*self.C, self.C)\n","        # then re layer norm on the number of channels\n","        self.norm = nn.LayerNorm(self.C)\n","\n","    def forward(self, x):\n","        # reduced the sptial scale of x\n","        # by reshaping the sequence into size HW/R^2 X R^2C at stage i\n","\n","        reduced_x = torch.reshape(\n","            x, [self.B, self.H*self.W//(self.R**2), (self.R**2*self.C)])\n","        new_x = self.linear_projection(reduced_x)\n","        new_x = self.norm(new_x)\n","        # output should be of size HW/R^2 x CI\n","\n","        return new_x\n","\n","\n","class Feed_Forward(nn.Module):\n","    def __init__(self, in_size, hidden_size, out_size):\n","        super().__init__()\n","        self.l1 = nn.Linear(in_size, hidden_size)\n","        self.relu = nn.ReLU(inplace=False)\n","        self.l2 = nn.Linear(hidden_size, out_size)\n","\n","    def forward(self, x):\n","        x = self.l1(x)\n","        x = self.relu(x)\n","        x = self.l2(x)\n","        return x\n","\n","\n","class Transformer_Encoder(nn.Module):\n","    def __init__(self, height, width, channels, reduction_ratio, patch_dim, batch_size, num_heads):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.norm1 = nn.LayerNorm(channels)\n","        self.a = SRAttention(self.num_heads, channels,\n","                             height//patch_dim, width//patch_dim, reduction_ratio, batch_size)\n","        self.norm2 = nn.LayerNorm(channels)\n","        self.ff = Feed_Forward(channels, channels//2, channels)\n","\n","    def forward(self, x):\n","        n1 = self.norm1(x)\n","        a = self.a(n1, n1, n1)\n","        x = torch.add(x, a)\n","        n2 = self.norm2(x)\n","        ff = self.ff(n2)\n","        x = torch.add(x, ff)\n","        return x\n","\n","\n","class Stage_Module(nn.Module):\n","    def __init__(self, channels, embedding_dim, Height, Width, reduction_ratio, patch_dim, batch_size, num_heads):\n","        super().__init__()\n","        self.H = Height\n","        self.W = Width\n","        self.out_dim = embedding_dim\n","        self.P = patch_dim\n","        self.B = batch_size\n","        self.PE = Patch_Embedding(channels, embedding_dim, patch_dim)\n","        self.TE = Transformer_Encoder(\n","            Height, Width, embedding_dim, reduction_ratio, patch_dim, batch_size, num_heads)\n","\n","    def forward(self, x):\n","        x = self.PE(x)\n","        x = self.TE(x)\n","        # # # reshape to H(i-1)/P x W(i-1)/P x ED as output\n","        x = torch.reshape(x, [self.B, self.H//self.P,\n","                              self.W//self.P, self.out_dim]).permute([0, 3, 1, 2])\n","        return x\n","\n","class classification_pvt(nn.Module):\n","    def __init__(self, channels, height, width, batch_size, num_classes):\n","        super().__init__()\n","        # input at stage 1 is H X W X 3\n","\n","        self.output_H = height//32\n","        self.output_W = width//32\n","\n","        self.stg1 = Stage_Module(channels, 64, height,\n","                                 width, reduction_ratio=8, patch_dim=4, batch_size=batch_size, num_heads=1)\n","        self.stg2 = Stage_Module(\n","            64, 128, height//4, width//4, reduction_ratio=4, patch_dim=2, batch_size=batch_size, num_heads=2)\n","        self.stg3 = Stage_Module(\n","            128, 256, height//8, width//8, reduction_ratio=2, patch_dim=2, batch_size=batch_size, num_heads=4)\n","        self.stg4 = Stage_Module(256, 512, height//16,\n","                                 width//16, reduction_ratio=1, patch_dim=2, batch_size=batch_size, num_heads=8)\n","\n","        self.head = nn.Linear(self.output_H*self.output_W*512, 128)\n","        self.head2 = nn.Linear(128, num_classes)\n","        self.relu = nn.ReLU(inplace=False)\n","\n","    def forward(self, x):\n","\n","        x = self.stg1(x)\n","\n","        x = self.stg2(x)\n","\n","        x = self.stg3(x)\n","\n","        x = self.stg4(x).permute([0, 2, 3, 1])\n","\n","        x = x.view(-1, self.output_H*self.output_W*512)\n","        x = self.head(x)\n","        x = self.relu(x)\n","        x = self.head2(x)\n","        return x"],"metadata":{"id":"Qd7BOvkpA377"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **CIFAR100**"],"metadata":{"id":"6lxEMq6pX1l_"}},{"cell_type":"code","source":["\n","transform = transforms.Compose(\n","    [transforms.Resize((224, 224)),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n","                                       download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n","                                         shuffle=False, num_workers=2, drop_last=True)\n","\n","testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n","                                         shuffle=False, num_workers=2, drop_last=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"486zmhyZgNDe","executionInfo":{"status":"ok","timestamp":1681673518355,"user_tz":240,"elapsed":6427,"user":{"displayName":"Winston Chan","userId":"18037746318607646840"}},"outputId":"2f90d4bf-addd-46af-d0b4-7f51da77d47e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 169001437/169001437 [00:02<00:00, 78194294.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"Y69P5uQPA9C1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681673571767,"user_tz":240,"elapsed":15398,"user":{"displayName":"Winston Chan","userId":"18037746318607646840"}},"outputId":"beb25ab8-223f-417c-97bd-682d8a28f3ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","net = classification_pvt(3, 224, 224, 1, 100)\n","# net.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/ckpt_class/cifar100_new_params.pth', map_location=torch.device('cpu')))\n","net.load_state_dict(torch.load('../ckpt_class/cifar100_new_params.pth', map_location=torch.device('cpu')))\n","\n","net.to(device)"],"metadata":{"id":"los4qODoA90Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Testing Accuracy\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Testing accuracy: {100 * correct // total} %')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"no7I5qjKdXRa","executionInfo":{"status":"ok","timestamp":1681669328615,"user_tz":240,"elapsed":497462,"user":{"displayName":"Winston Chan","userId":"18037746318607646840"}},"outputId":"3f6111cd-c891-4123-ae47-fea56b2d5740"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing accuracy: 31 %\n"]}]},{"cell_type":"code","source":["### Testing Top-1 Error\n","net.eval()\n","\n","top1_error = 0.0\n","\n","for inputs, labels in testloader:\n","    outputs = net(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    incorrect = (predicted != labels)\n","    top1_error += incorrect.sum().item()\n","\n","print(f'Top-1 Error: {100*(top1_error/10000)} %')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Sb9EINlRfLT","executionInfo":{"status":"ok","timestamp":1681670164099,"user_tz":240,"elapsed":546983,"user":{"displayName":"Winston Chan","userId":"18037746318607646840"}},"outputId":"0db69043-7949-4c55-da6e-0ecb70a2b024"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top-1 Error: 68.38 %\n"]}]},{"cell_type":"code","source":["### Training Top-1 Error\n","net.eval()\n","\n","top1_error = 0.0\n","\n","for inputs, labels in trainloader:\n","    outputs = net(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    incorrect = (predicted != labels)\n","    top1_error += incorrect.sum().item()\n","\n","print(f'Top-1 Error: {100*(top1_error/50000)} %')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPxIZnbjaJT0","executionInfo":{"status":"ok","timestamp":1681674447920,"user_tz":240,"elapsed":833817,"user":{"displayName":"Winston Chan","userId":"18037746318607646840"}},"outputId":"6b77ad8f-0de7-45b3-aa2e-385d06764c8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top-1 Error: 0.064 %\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"J5PGN4AXamXw"},"execution_count":null,"outputs":[]}]}